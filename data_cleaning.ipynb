{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./env/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in ./env/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./env/lib/python3.11/site-packages (from pandas) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: et-xmlfile in ./env/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_file = 'texts.xlsx'\n",
    "sheet_name = '2010 - 2014'\n",
    "df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def extract_domain(url):\n",
    "    try:\n",
    "        return urlparse(url).netloc\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "df['Domain'] = df['URLs'].apply(extract_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singapore Business Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Domain'] = df['URLs'].apply(extract_domain)\n",
    "filtered_df = df[df['Domain'] == 'sbr.com.sg']\n",
    "\n",
    "# remove heading\n",
    "remove_text = \"Singapore Business Review website works best with Javascript enabled. Please enable your javascript and reload the page.\"\n",
    "filtered_df.loc[:, 'Text'] = filtered_df['Text'].str.replace(f'^{remove_text}', '', regex=True)\n",
    "# remove ending\n",
    "remove_text = (\"..there are many ways you can work with us to advertise your company and connect to your customers.\"\n",
    "               \"Our team can help you dight and create an advertising campaign, in print and digital, on this website and in print magazine.\"\n",
    "               \"We can also organize a real life or digital event for you and find thought leader speakers as well as industry leaders, who could be your potential partners, to join the event.\"\n",
    "               \"We also run some awards programmes which give you an opportunity to be recognized for your achievements during the year and you can join this as a participant or a sponsor.\"\n",
    "               \"Let us help you drive your business forward with a good partnership!\"\n",
    "               \"Copyright 2024 Charlton Media Group.\"\n",
    "               \"Web Design by: Halcyon Web DesignCopyright 2024 Charlton Media Group.Web Design by: Halcyon Web Design\")\n",
    "\n",
    "filtered_df.loc[:, 'Text'] = filtered_df['Text'].str.replace(remove_text, '')\n",
    "\n",
    "filtered_df.to_excel('data_cleaning.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to texts.xlsx\n",
    "cleaned_df = pd.DataFrame(filtered_df)\n",
    "cleaned_dict = cleaned_df.set_index('URLs')['Text'].to_dict()\n",
    "\n",
    "df['Text'] = df.apply(\n",
    "    lambda row: cleaned_dict.get(row['URLs'], row['Text']), axis=1\n",
    ")\n",
    "\n",
    "df = df.drop(columns=['Domain'])\n",
    "\n",
    "with pd.ExcelWriter('texts_cleaned.xlsx', engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Domain'] = df['URLs'].apply(extract_domain)\n",
    "filtered_df = df[df['Domain'] == 'www.businesstimes.com.sg']\n",
    "\n",
    "# remove heading\n",
    "remove_text = \"Login\"\n",
    "filtered_df.loc[:, 'Text'] = filtered_df['Text'].str.replace(f'^{remove_text}', '', regex=True)\n",
    "# remove the last 301 characters\n",
    "filtered_df.loc[:, 'Text'] = filtered_df['Text'].str[:-301]\n",
    "\n",
    "filtered_df.to_excel('data_cleaning.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to texts.xlsx\n",
    "cleaned_df = pd.DataFrame(filtered_df)\n",
    "cleaned_dict = cleaned_df.set_index('URLs')['Text'].to_dict()\n",
    "\n",
    "df['Text'] = df.apply(\n",
    "    lambda row: cleaned_dict.get(row['URLs'], row['Text']), axis=1\n",
    ")\n",
    "\n",
    "df = df.drop(columns=['Domain'])\n",
    "\n",
    "with pd.ExcelWriter('texts_cleaned.xlsx', engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straits Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Domain'] = df['URLs'].apply(extract_domain)\n",
    "filtered_df = df[df['Domain'] == 'www.straitstimes.com']\n",
    "\n",
    "remove_text = (\"Join ST's Telegram channel and get the latest breaking news delivered to you.\"\n",
    "               \"Read 3 articles and stand to win rewards\"\n",
    "               \"Spin the wheel now\"\n",
    "               \"MCI (P) 066/10/2023. Published by SPH Media Limited, Co. Regn. No. 202120748H. \"\n",
    "               \"Copyright 2024 SPH Media Limited. All rights reserved.\")\n",
    "\n",
    "filtered_df.loc[:, 'Text'] = filtered_df['Text'].str.replace(remove_text, '')\n",
    "\n",
    "filtered_df.to_excel('data_cleaning.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to texts.xlsx\n",
    "cleaned_df = pd.DataFrame(filtered_df)\n",
    "cleaned_dict = cleaned_df.set_index('URLs')['Text'].to_dict()\n",
    "\n",
    "df['Text'] = df.apply(\n",
    "    lambda row: cleaned_dict.get(row['URLs'], row['Text']), axis=1\n",
    ")\n",
    "\n",
    "df = df.drop(columns=['Domain'])\n",
    "\n",
    "with pd.ExcelWriter('texts_cleaned.xlsx', engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel News Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Domain'] = df['URLs'].apply(extract_domain)\n",
    "filtered_df = df[df['Domain'] == 'www.channelnewsasia.com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EdgeProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_text(urls):\n",
    "    results = []\n",
    "    for index, url in enumerate(urls):\n",
    "        print(f\"processing URL {index}: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                text_blocks = soup.find_all('div', class_='jsx-213751841 truncated_textview_box')\n",
    "                    \n",
    "                if text_blocks:\n",
    "                    text = ' '.join(block.get_text(strip=True) for block in text_blocks)\n",
    "                    results.append({'URLs': url, 'Text': text})\n",
    "            else:\n",
    "                results.append({'URLs': url, 'Text': 'failed to retrieve content'})\n",
    "        except Exception as e:\n",
    "            results.append({'URLs': url, 'Text': str(e)})\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing URL 0: https://www.edgeprop.sg/property-news/sitting-goldmine-%E2%80%93-landed-housing-segment\n",
      "processing URL 1: https://www.edgeprop.sg/property-news/competition-heats-farrer-road-leedon-road\n",
      "processing URL 2: https://www.edgeprop.sg/property-news/ho-bee%E2%80%99s-metropolis\n",
      "processing URL 3: https://www.edgeprop.sg/property-news/new-lease-%E2%80%98awesomeness%E2%80%99\n",
      "processing URL 4: https://www.edgeprop.sg/property-news/good-class-bungalow-35-mil\n",
      "processing URL 5: https://www.edgeprop.sg/property-news/prudential-tower-strata-unit-sale-2800-psf\n",
      "processing URL 6: https://www.edgeprop.sg/property-news/jewel-rangoon-road\n",
      "processing URL 7: https://www.edgeprop.sg/property-news/shadow-d%E2%80%99leedon\n",
      "processing URL 8: https://www.edgeprop.sg/property-news/pricing-shock-iskandar%E2%80%99s-puteri-harbour\n"
     ]
    }
   ],
   "source": [
    "df['Domain'] = df['URLs'].apply(extract_domain)\n",
    "filtered_df = df[df['Domain'] == 'www.edgeprop.sg']\n",
    "\n",
    "urls = filtered_df['URLs'].tolist()\n",
    "scraped_df = scrape_text(urls)\n",
    "scraped_df.to_excel('data_cleaning.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to texts.xlsx\n",
    "cleaned_df = pd.DataFrame(scraped_df)\n",
    "cleaned_dict = cleaned_df.set_index('URLs')['Text'].to_dict()\n",
    "\n",
    "df['Text'] = df.apply(\n",
    "    lambda row: cleaned_dict.get(row['URLs'], row['Text']), axis=1\n",
    ")\n",
    "\n",
    "df = df.drop(columns=['Domain'])\n",
    "\n",
    "with pd.ExcelWriter('texts_cleaned.xlsx', engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 URLs  \\\n",
      "0   https://sbr.com.sg/information-technology/more...   \n",
      "1   https://www.businesstimes.com.sg/incoming/ura-...   \n",
      "2   https://www.businesstimes.com.sg/incoming/hdb-...   \n",
      "3   https://www.edgeprop.sg/property-news/sitting-...   \n",
      "4   https://www.businesstimes.com.sg/incoming/lawm...   \n",
      "..                                                ...   \n",
      "73  https://www.edgeprop.sg/property-news/good-cla...   \n",
      "74  https://www.edgeprop.sg/property-news/prudenti...   \n",
      "75  https://www.edgeprop.sg/property-news/jewel-ra...   \n",
      "76  https://www.edgeprop.sg/property-news/shadow-d...   \n",
      "77  https://www.edgeprop.sg/property-news/pricing-...   \n",
      "\n",
      "                         Date  \\\n",
      "0    2010-05-21T20:30:40+0800   \n",
      "1   2013-11-25T22:00:00+08:00   \n",
      "2   2013-10-14T22:00:00+08:00   \n",
      "3   2014-03-01T00:00:00+08:00   \n",
      "4   2014-03-04T22:00:00+08:00   \n",
      "..                        ...   \n",
      "73  2014-10-13T00:00:00+08:00   \n",
      "74  2014-10-20T00:00:00+08:00   \n",
      "75  2014-10-27T10:50:58+08:00   \n",
      "76  2014-11-24T15:51:30+08:00   \n",
      "77  2014-10-20T13:31:07+08:00   \n",
      "\n",
      "                                                 Text  \n",
      "0   IBM's fourth high-end systems manufacturing fa...  \n",
      "1   It says they should clarify approved use of a ...  \n",
      "2   SingaporeTHE Housing & Development Board (HDB)...  \n",
      "3   The private landed residential segment has out...  \n",
      "4   Rising industrial, commercial rents cited as k...  \n",
      "..                                                ...  \n",
      "73  SINGAPORE: A rare property on Rebecca Road wit...  \n",
      "74  SINGAPORE: The 30-storey Prudential Tower offi...  \n",
      "75  SINGAPORE:For more than a decade, entrepreneur...  \n",
      "76  SINGAPORE:The completion of the project, Singa...  \n",
      "77  SINGAPORE: On the weekend of Sept 20 and 21, d...  \n",
      "\n",
      "[78 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "excel_file = 'texts_cleaned.xlsx'\n",
    "sheet_name = '2010 - 2014'\n",
    "df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "\n",
    "# remove leading and trailing whitespaces\n",
    "df['Text'] = df['Text'].str.strip()\n",
    "print(df)\n",
    "\n",
    "# replace multiple spaces between words with a single space\n",
    "df.loc[:, 'Text'] = df['Text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "with pd.ExcelWriter(excel_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
