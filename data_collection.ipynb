{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Collecting fake_useragent\n",
      "  Using cached fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Using cached numpy-2.0.1-cp311-cp311-macosx_11_0_arm64.whl (13.3 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, fake_useragent, urllib3, tzdata, soupsieve, numpy, idna, et-xmlfile, charset-normalizer, certifi, requests, pandas, openpyxl, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 certifi-2024.7.4 charset-normalizer-3.3.2 et-xmlfile-1.1.0 fake_useragent-1.5.1 idna-3.7 numpy-2.0.1 openpyxl-3.1.5 pandas-2.2.2 pytz-2024.1 requests-2.32.3 soupsieve-2.5 tzdata-2024.1 urllib3-2.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 fake_useragent pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import urllib.parse\n",
    "\n",
    "def google_search(query, num_results, time_filter = None):\n",
    "    ua = UserAgent()\n",
    "    headers = {'User-Agent': ua.random}\n",
    "\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "\n",
    "    google_url = f\"https://www.google.com/search?q={query}&num={num_results}\"\n",
    "\n",
    "    if time_filter:\n",
    "        google_url += f\"&tbs={time_filter}\"\n",
    "\n",
    "    response = requests.get(google_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        search_results = []\n",
    "\n",
    "        for g in soup.find_all('div', class_='g'):\n",
    "            anchors = g.find_all('a')\n",
    "            if anchors:\n",
    "                link = anchors[0]['href']\n",
    "                search_results.append(link)\n",
    "                \n",
    "        return search_results\n",
    "    else:\n",
    "        print(f\"failed to retrieve search results: status code {response.status_code}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quarters(start_year, end_year):\n",
    "    quarters = {}\n",
    "    if end_year == 2024:\n",
    "        quarters[\"2024 Q1\"] = \"cdr:1,cd_min:1/1/2024,cd_max:3/31/2024\"\n",
    "        quarters[\"2024 Q2\"] = \"cdr:1,cd_min:4/1/2024,cd_max:6/30/2024\"\n",
    "        end_year -= 1\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        quarters[f\"{year} Q1\"] = f\"cdr:1,cd_min:1/1/{year},cd_max:3/31/{year}\"\n",
    "        quarters[f\"{year} Q2\"] = f\"cdr:1,cd_min:4/1/{year},cd_max:6/30/{year}\"\n",
    "        quarters[f\"{year} Q3\"] = f\"cdr:1,cd_min:7/1/{year},cd_max:9/30/{year}\"\n",
    "        quarters[f\"{year} Q4\"] = f\"cdr:1,cd_min:10/1/{year},cd_max:12/31/{year}\"\n",
    "    return quarters\n",
    "\n",
    "def generate_query(source_list):\n",
    "    dictionary = {}\n",
    "    for source in source_list:\n",
    "        if source in dictionary:\n",
    "            continue\n",
    "        else:\n",
    "            dictionary[source] = f\"singapore industrial property market {source}\"\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  URLs  \\\n",
      "0    https://www.channelnewsasia.com/singapore/mark...   \n",
      "1          https://www.youtube.com/watch?v=JsMbvmY9FWA   \n",
      "2    https://www.channelnewsasia.com/singapore/joho...   \n",
      "3          https://www.youtube.com/watch?v=iJ2Xfc0xnZk   \n",
      "4    https://m.facebook.com/ChannelNewsAsia/posts/8...   \n",
      "..                                                 ...   \n",
      "432  https://plbinsights.com/the-rise-of-self-stora...   \n",
      "433  https://www.propertyguru.com.sg/property-guide...   \n",
      "434  https://sbr.com.sg/commercial-property/in-focu...   \n",
      "435  https://realestateasia.com/videos/proptech-kuc...   \n",
      "436  https://cosysingapore.com/commercial-property-...   \n",
      "\n",
      "                        Source  Quarter  \n",
      "0                          cna  2024 Q1  \n",
      "1                          cna  2024 Q1  \n",
      "2                          cna  2024 Q1  \n",
      "3                          cna  2024 Q1  \n",
      "4                          cna  2024 Q1  \n",
      "..                         ...      ...  \n",
      "432  singapore business review  2023 Q3  \n",
      "433  singapore business review  2023 Q3  \n",
      "434  singapore business review  2023 Q3  \n",
      "435  singapore business review  2023 Q3  \n",
      "436  singapore business review  2023 Q3  \n",
      "\n",
      "[437 rows x 3 columns]\n",
      "                                                   URLs    Source  Quarter\n",
      "0     https://www.channelnewsasia.com/singapore/mark...       cna  2024 Q1\n",
      "1           https://www.youtube.com/watch?v=JsMbvmY9FWA       cna  2024 Q1\n",
      "2     https://www.channelnewsasia.com/singapore/joho...       cna  2024 Q1\n",
      "3           https://www.youtube.com/watch?v=iJ2Xfc0xnZk       cna  2024 Q1\n",
      "4     https://m.facebook.com/ChannelNewsAsia/posts/8...       cna  2024 Q1\n",
      "...                                                 ...       ...      ...\n",
      "1180  https://www.edgeprop.sg/industrial/kewalram-house  edgeprop  2022 Q4\n",
      "1181     https://asianprimeproperties.sg/property-news/  edgeprop  2022 Q4\n",
      "1182  https://www.theedgesingapore.com/billion-dolla...  edgeprop  2022 Q4\n",
      "1183  https://static1.squarespace.com/static/5909917...  edgeprop  2022 Q4\n",
      "1184  https://www.propertyguru.com.sg/agent/sharon-k...  edgeprop  2022 Q4\n",
      "\n",
      "[1185 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query_dictionary = generate_query([\"cna\", \"singapore business review\"])\n",
    "\n",
    "quarter_dictionary = generate_quarters(2020, 2024)\n",
    "\n",
    "headers = [\"URLs\", \"Source\", \"Quarter\"]\n",
    "df = pd.DataFrame(columns=headers)\n",
    "\n",
    "for source, query in query_dictionary.items():\n",
    "    for quarter, time_filter in quarter_dictionary.items():\n",
    "        results = google_search(query, num_results=20, time_filter=time_filter)\n",
    "        temp_df = pd.DataFrame({\"URLs\": results, \"Source\": source, \"Quarter\": quarter})\n",
    "        df = pd.concat([df, temp_df], ignore_index=True)\n",
    "\n",
    "print(df)\n",
    "\n",
    "query_dictionary = generate_query([\"straits times\", \"business times\", \"edgeprop\"])\n",
    "\n",
    "quarter_dictionary = generate_quarters(2020, 2024)\n",
    "\n",
    "for source, query in query_dictionary.items():\n",
    "    for quarter, time_filter in quarter_dictionary.items():\n",
    "        results = google_search(query, num_results=30, time_filter=time_filter)\n",
    "        temp_df = pd.DataFrame({\"URLs\": results, \"Source\": source, \"Quarter\": quarter})\n",
    "        df = pd.concat([df, temp_df], ignore_index=True)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_excel(\"urls.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"urls.xlsx\")\n",
    "duplicates = df[\"URLs\"].duplicated(keep=False)\n",
    "df = df[~duplicates]\n",
    "\n",
    "with pd.ExcelWriter(\"urls.xlsx\", engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    df.to_excel(writer, sheet_name=\"2020 - 2024\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Scraping URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_article_date(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"failed to retrieve the webpage: {e}\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # locate the date in the meta tag with property 'article:published_time'\n",
    "    date = soup.find('meta', attrs={'property': 'article:published_time'})\n",
    "    if date:\n",
    "        return date['content']\n",
    "    \n",
    "    # locate the date in the meta tag with name 'cXenseParse:recs:publishtime'\n",
    "    date = soup.find('meta', attrs={'name': 'cXenseParse:recs:publishtime'})\n",
    "    if date:\n",
    "        return date['content']\n",
    "    \n",
    "    # locate the date in the meta tag with name 'article:published_time'\n",
    "    date = soup.find('meta', attrs={'name': 'article:published_time'})\n",
    "    if date:\n",
    "        return date['content']\n",
    "    \n",
    "    # locate the date \n",
    "    time_element = soup.find('time')\n",
    "    if time_element and 'datetime' in time_element.attrs:\n",
    "        return time_element['datetime']\n",
    "    \n",
    "    # if not found\n",
    "    return \"date not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-10T07:24:00+08:00\n",
      "2024-04-12T07:37:00+08:00\n",
      "2024-05-23T08:00:00+08:00\n",
      "2024-04-26T13:00:00+08:00\n",
      "2024-06-25T19:40:00+08:00\n",
      "2024-04-17T13:34:20+08:00\n",
      "2024-05-26T05:00:00+08:00\n",
      "2024-06-28T07:25:00+08:00\n",
      "2024-04-18T07:25:00+08:00\n",
      "2024-05-28T19:10:00+08:00\n",
      "2024-06-19T08:39:00+08:00\n",
      "date not found\n",
      "2024-04-25T18:30:00+08:00\n",
      "2024-04-04T05:00:00+08:00\n",
      "2024-04-26T17:19:24+08:00\n",
      "2024-04-25T13:29:11+08:00\n",
      "2024-06-27T08:44:22+08:00\n",
      "2024-06-03T11:59:01+08:00\n",
      "2024-05-29T22:26:22+08:00\n",
      "2024-04-28T17:36:06+08:00\n",
      "2024-05-08T10:08:52+08:00\n",
      "2024-06-16T17:07:29+08:00\n",
      "2024-05-14T05:00:00+0800\n",
      "2024-02-07T15:37:42+08:00\n",
      "2024-03-11T18:22:47+08:00\n",
      "2024-03-20T13:50:57+08:00\n",
      "2024-02-06T15:51:56+08:00\n",
      "2024-03-27T17:18:29+08:00\n",
      "2024-01-26T02:17:16+08:00\n",
      "2024-01-05T16:40:00+08:00\n",
      "2024-01-05T13:08:15+08:00\n",
      "2024-03-07T14:48:03+08:00\n",
      "2024-01-27T15:00:00+08:00\n",
      "2024-01-12T14:00:00+08:00\n",
      "2024-03-05T12:53:00+08:00\n",
      "2024-01-02T08:53:00+08:00\n",
      "2024-01-19T17:47:38+08:00\n",
      "2024-03-20T15:14:00+08:00\n",
      "2024-01-13T11:25:00+08:00\n",
      "2024-03-28T19:04:24+08:00\n",
      "2024-01-15T17:17:00+08:00\n",
      "2024-03-27T16:08:46+08:00\n",
      "2024-01-05T16:23:35+08:00\n",
      "2024-02-27T13:54:34+08:00\n",
      "2024-01-25T16:37:03+0800\n",
      "2024-02-14T05:00:00+0800\n",
      "2024-01-23T10:30:38+0800\n",
      "2024-03-26T12:37:24+0800\n",
      "2024-02-02T05:00:00+0800\n",
      "2023-10-03T14:29:33+08:00\n",
      "2023-12-13T05:00:00+08:00\n",
      "2023-10-16T10:24:53+08:00\n",
      "2023-10-26T16:00:00+08:00\n",
      "2023-10-16T08:00:00+08:00\n",
      "2023-12-11T07:14:11+08:00\n",
      "2023-11-22T08:00:00+08:00\n",
      "2023-11-01T19:56:00+08:00\n",
      "2023-11-28T20:45:00+08:00\n",
      "2023-10-12T22:40:00+08:00\n",
      "2023-11-09T09:32:00+08:00\n",
      "2023-12-19T10:00:00+08:00\n",
      "2023-12-25T05:00:00+08:00\n",
      "2023-10-16T08:41:32+08:00\n",
      "2023-11-21T14:58:53+08:00\n",
      "2023-10-27T09:58:36+08:00\n",
      "2023-10-27T08:42:33+08:00\n",
      "2023-12-28T22:20:58+08:00\n",
      "2023-10-19T18:45:17+08:00\n",
      "2023-10-31T20:45:13+08:00\n",
      "2023-10-26T16:57:05+08:00\n",
      "2023-12-27T13:42:25+08:00\n",
      "2023-12-17T20:26:03+08:00\n",
      "2023-12-11T07:01:55+08:00\n",
      "2023-08-18T21:20:00+08:00\n",
      "2023-08-03T22:21:43+08:00\n",
      "2023-09-24T05:00:00+08:00\n",
      "2023-09-26T13:53:26+08:00\n",
      "2023-08-01T14:45:00+08:00\n",
      "2023-08-14T04:00:00+08:00\n",
      "2023-07-19T04:00:00+08:00\n",
      "2023-09-25T07:51:21+08:00\n",
      "2023-08-07T12:01:00+08:00\n",
      "2023-09-06T21:47:00+08:00\n",
      "2023-08-29T18:00:00+08:00\n",
      "2023-08-31T16:32:40+08:00\n",
      "2023-07-21T11:30:00+08:00\n",
      "2023-07-05T15:27:54+08:00\n",
      "2023-07-04T09:43:47+08:00\n",
      "2023-09-28T05:00:00+08:00\n",
      "2023-09-28T05:00:00+08:00\n",
      "2023-08-31T05:00:00+0800\n",
      "2023-08-07T11:27:58+0800\n",
      "2023-09-12T05:00:00+0800\n",
      "2023-09-01T05:00:00+0800\n",
      "2023-08-29T11:40:18+0800\n",
      "2023-05-10T21:45:00+08:00\n",
      "2023-05-25T05:00:00+08:00\n",
      "2023-04-10T15:57:36+08:00\n",
      "2023-04-28T17:31:06+08:00\n",
      "date not found\n",
      "2023-04-07T15:19:56+08:00\n",
      "2023-05-30T09:25:03+08:00\n",
      "2023-05-29T05:50:00+08:00\n",
      "2023-04-28T09:30:06+08:00\n",
      "2023-04-27T13:00:00+08:00\n",
      "2023-06-21T09:53:06+08:00\n",
      "2023-06-14T05:50:00+08:00\n",
      "2023-04-20T18:05:43+08:00\n",
      "2023-05-07T17:36:36+08:00\n",
      "2023-05-24T05:50:00+08:00\n",
      "2023-03-09T05:00:00+08:00\n",
      "2023-02-14T19:20:55+08:00\n",
      "2023-02-15T13:01:20+08:00\n",
      "2023-01-27T11:36:55+08:00\n",
      "2023-03-14T05:00:00+08:00\n",
      "2023-01-19T17:51:37+08:00\n",
      "failed to retrieve the webpage: Response ended prematurely\n",
      "2023-02-04T17:21:00+08:00\n",
      "2023-02-14T22:42:00+08:00\n",
      "2023-01-10T06:00:00+08:00\n",
      "2023-02-08T06:00:00+08:00\n",
      "2023-01-12T16:18:01+08:00\n",
      "2023-02-23T20:00:00+08:00\n",
      "2023-02-28T06:00:00+08:00\n",
      "2023-01-03T08:00:00+08:00\n",
      "2023-02-24T14:52:32+08:00\n",
      "2022-10-18T04:00:00+08:00\n",
      "2022-12-08T13:24:48+08:00\n",
      "2022-10-31T12:24:27+08:00\n",
      "2022-10-21T10:42:02+08:00\n",
      "2022-12-09T12:49:49+08:00\n",
      "2022-10-21T12:00:00+08:00\n",
      "2022-11-10T18:28:18+08:00\n",
      "2022-10-18T16:42:31+08:00\n",
      "2022-11-11T09:00:00+08:00\n",
      "2022-11-29T16:16:47+08:00\n",
      "2022-10-04T05:50:00+08:00\n",
      "2022-10-31T05:50:00+08:00\n",
      "2022-10-06T05:50:00+08:00\n",
      "2022-10-06T05:50:00+08:00\n",
      "2022-10-06T05:50:00+08:00\n",
      "2022-10-06T05:50:00+08:00\n",
      "2022-11-09T22:17:34+08:00\n",
      "2022-10-27T13:42:17+08:00\n",
      "2022-12-14T05:50:00+08:00\n",
      "2022-12-28T17:03:24+08:00\n",
      "2022-07-11T05:00:00+08:00\n",
      "2022-08-10T15:33:00+08:00\n",
      "2022-08-19T15:00:00+08:00\n",
      "2022-09-06T06:06:00+08:00\n",
      "2022-08-22T06:11:00+08:00\n",
      "2022-09-09T13:03:00+08:00\n",
      "2022-08-18T19:34:51+08:00\n",
      "2022-07-18T12:45:39+08:00\n",
      "2022-09-05T11:26:21+08:00\n",
      "2022-07-13T10:39:06+08:00\n",
      "2022-07-27T12:45:46+08:00\n",
      "2022-07-12T22:50:38+08:00\n",
      "2022-07-01T19:46:17+08:00\n",
      "2022-08-10T05:50:00+08:00\n",
      "2022-07-27T05:50:00+08:00\n",
      "2022-07-26T05:00:00+0800\n",
      "2022-09-30T16:02:35+0800\n",
      "2022-09-14T00:00:00+0800\n",
      "2022-06-06T12:00:00+08:00\n",
      "2022-06-30T14:41:06+08:00\n",
      "2022-05-26T21:07:00+08:00\n",
      "2022-04-07T20:09:00+08:00\n",
      "2022-04-12T17:01:00+08:00\n",
      "2022-05-24T11:30:00+08:00\n",
      "2022-06-29T05:05:07+0800\n",
      "2022-04-11T14:39:39+0800\n",
      "2022-06-22T10:46:42+0800\n",
      "2022-05-12T10:00:00+0800\n",
      "2022-02-01T05:00:00+08:00\n",
      "2022-01-24T17:06:40+08:00\n",
      "2022-03-28T18:04:40+08:00\n",
      "2022-02-23T14:40:22+08:00\n",
      "2022-01-24T15:58:36+08:00\n",
      "2022-03-14T14:00:09+08:00\n",
      "2022-02-28T22:39:55+08:00\n",
      "2022-03-15T07:16:57+08:00\n",
      "2022-01-11T17:30:00+08:00\n",
      "2022-01-27T14:00:00+08:00\n",
      "2022-02-23T14:58:00+08:00\n",
      "2022-02-28T06:03:41+08:00\n",
      "failed to retrieve the webpage: ('Connection broken: IncompleteRead(14773 bytes read, 1 more expected)', IncompleteRead(14773 bytes read, 1 more expected))\n",
      "2022-01-26T12:56:00+08:00\n",
      "2022-01-19T06:06:00+08:00\n",
      "2022-03-15T21:50:00+08:00\n",
      "2022-02-14T05:31:44+08:00\n",
      "2022-01-16T04:29:02+08:00\n",
      "2022-03-30T21:50:00+08:00\n",
      "2022-01-10T01:16:49+08:00\n",
      "2022-02-22T03:11:34+08:00\n",
      "2022-02-22T21:50:00+08:00\n",
      "2022-02-11T03:26:30+08:00\n",
      "2022-03-08T10:12:37+0800\n",
      "2021-12-24T01:31:17+08:00\n",
      "2021-12-27T05:30:00+08:00\n",
      "2021-12-06T06:00:34+08:00\n",
      "2021-12-09T06:02:28+08:00\n",
      "2021-11-27T06:00:00+08:00\n",
      "2021-10-28T07:49:54+08:00\n",
      "2021-11-18T10:24:00+08:00\n",
      "2021-12-23T08:44:00+08:00\n",
      "2021-12-06T09:21:58+0800\n",
      "2021-07-20T05:00:00+08:00\n",
      "2021-08-30T20:14:50+08:00\n",
      "2021-08-02T18:56:09+08:00\n",
      "2021-07-22T22:12:26+08:00\n",
      "2021-07-05T16:26:18+08:00\n",
      "2021-08-26T06:06:12+08:00\n",
      "2021-08-14T06:00:00+08:00\n",
      "2021-08-24T06:30:00+08:00\n",
      "2021-08-20T19:56:32+08:00\n",
      "2021-09-28T08:30:00+08:00\n",
      "2021-09-06T08:48:19+08:00\n",
      "2021-09-02T03:07:40+08:00\n",
      "2021-09-14T11:08:01+0800\n",
      "2021-09-21T05:00:00+0800\n",
      "2021-09-14T08:54:37+0800\n",
      "2021-04-16T22:30:39+08:00\n",
      "2021-06-06T05:00:00+08:00\n",
      "2021-04-07T15:18:01+08:00\n",
      "2021-05-07T10:57:47+08:00\n",
      "2021-06-28T13:31:55+08:00\n",
      "2021-04-15T12:38:42+08:00\n",
      "2021-06-29T20:27:12+08:00\n",
      "2021-04-27T21:24:42+08:00\n",
      "2021-06-01T12:24:05+08:00\n",
      "2021-06-02T18:21:56+08:00\n",
      "2021-06-04T08:00:00+08:00\n",
      "2021-05-07T06:00:00+08:00\n",
      "2021-05-06T07:17:08+08:00\n",
      "2021-06-08T05:30:04+08:00\n",
      "2021-01-27T04:00:00+08:00\n",
      "2021-03-10T09:41:07+08:00\n",
      "2021-03-10T12:32:12+08:00\n",
      "2021-01-17T18:06:18+08:00\n",
      "2021-03-22T13:06:35+08:00\n",
      "2021-01-27T23:02:56+08:00\n",
      "2021-01-19T09:25:07+08:00\n",
      "2021-02-05T06:30:00+08:00\n",
      "2021-01-11T15:45:57+08:00\n",
      "2021-02-16T17:47:00+08:00\n",
      "2021-03-27T06:01:28+08:00\n",
      "2021-03-09T06:01:20+08:00\n",
      "2021-03-29T06:00:27+08:00\n",
      "2021-03-11T21:50:00+08:00\n",
      "2021-01-18T21:50:00+08:00\n",
      "2021-03-31T21:50:00+08:00\n",
      "2021-03-31T21:50:00+08:00\n",
      "2021-03-25T08:01:42+08:00\n",
      "2021-03-16T08:23:30+08:00\n",
      "2021-02-10T21:50:00+08:00\n",
      "2021-03-17T08:17:36+08:00\n",
      "2021-01-12T21:50:00+08:00\n",
      "2021-02-02T01:49:56+08:00\n",
      "2021-03-18T10:54:31+0800\n",
      "2021-02-24T12:05:33+0800\n",
      "2021-02-11T10:54:25+0800\n",
      "2020-10-09T18:00:00+08:00\n",
      "2020-10-16T12:54:21+08:00\n",
      "2020-10-26T15:16:12+08:00\n",
      "2020-10-22T11:44:35+08:00\n",
      "2020-10-30T06:00:00+08:00\n",
      "2020-11-17T16:35:58+08:00\n",
      "2020-12-24T06:00:00+08:00\n",
      "2020-10-09T05:59:40+08:00\n",
      "2020-10-08T06:04:10+08:00\n",
      "2020-11-29T06:06:44+08:00\n",
      "2020-11-21T06:00:28+08:00\n",
      "2020-11-18T21:50:00+08:00\n",
      "2020-11-18T21:50:00+08:00\n",
      "2020-11-03T10:52:43+0800\n",
      "2020-08-14T09:50:50+0800\n",
      "2020-05-12T16:00:00+08:00\n",
      "2020-06-30T11:27:00+08:00\n",
      "2020-06-24T11:00:09+08:00\n",
      "2020-06-26T11:57:58+08:00\n",
      "2020-05-26T08:00:46+08:00\n",
      "2020-05-05T09:36:32+08:00\n",
      "2020-06-23T09:19:39+08:00\n",
      "2020-06-04T20:18:59+08:00\n",
      "2020-05-14T11:14:10+08:00\n",
      "2020-06-17T15:26:41+08:00\n",
      "2020-06-05T06:51:51+08:00\n",
      "2020-04-17T06:00:00+08:00\n",
      "2020-04-17T13:24:20+08:00\n",
      "2020-04-29T17:57:12+08:00\n",
      "2020-04-30T06:00:00+08:00\n",
      "2020-06-01T06:05:45+08:00\n",
      "2020-05-20T06:12:26+08:00\n",
      "2020-06-06T06:01:34+08:00\n",
      "2020-05-13T06:00:00+0800\n",
      "2020-01-07T13:48:02+08:00\n",
      "2020-02-18T13:44:53+08:00\n",
      "2020-02-05T13:05:36+08:00\n",
      "2020-02-17T16:31:53+08:00\n",
      "2020-01-09T14:57:15+08:00\n",
      "2020-03-20T08:08:08+08:00\n",
      "2020-03-13T15:34:45+08:00\n",
      "2020-02-12T11:47:01+08:00\n",
      "2020-02-06T21:50:00+08:00\n",
      "2020-01-29T09:04:34+08:00\n",
      "2020-01-08T14:24:50+0800\n",
      "2020-01-23T17:29:36+0800\n",
      "2020-03-23T06:00:00+0800\n"
     ]
    }
   ],
   "source": [
    "excel_file = 'urls.xlsx'\n",
    "sheet_name = '2020 - 2024'\n",
    "df_existing = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "\n",
    "urls = df_existing['URLs'].tolist()\n",
    "\n",
    "date_data = []\n",
    "for url in urls:\n",
    "    date = get_article_date(url)\n",
    "    print(date)\n",
    "    date_data.append({'URLs': url, 'Date': date})\n",
    "\n",
    "df_date = pd.DataFrame(date_data)\n",
    "\n",
    "df_updated = pd.merge(df_existing, df_date, on='URLs', how='left')\n",
    "\n",
    "with pd.ExcelWriter(excel_file, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    df_updated.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Source</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [URLs, Source, Quarter, Date]\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel_file = 'urls.xlsx'\n",
    "date_df = pd.read_excel(excel_file, sheet_name='2020 - 2024')\n",
    "\n",
    "filtered_df = date_df[date_df['Date'] == \"date not found\"]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "File \u001b[0;32m~/Desktop/Visual Code Studio/jtc-market-sentiments/myenv/lib/python3.11/site-packages/pandas/__init__.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m         _missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _missing_dependencies:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to import required dependencies:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_missing_dependencies)\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "excel_file = 'urls.xlsx'\n",
    "df = pd.read_excel(excel_file, sheet_name='2020 - 2024')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel NewsAsia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_cna \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mloc[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcna\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m urls \u001b[38;5;241m=\u001b[39m df_cna[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURLs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      4\u001b[0m cna_data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_cna = df.loc[df['Source'] == \"cna\"]\n",
    "urls = df_cna['URLs'].tolist()\n",
    "\n",
    "cna_data = []\n",
    "\n",
    "for index, url in enumerate(urls):\n",
    "    print(f\"processing URL {index + 1}/{len(urls)}: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            paragraphs = soup.find_all('p')\n",
    "            extracted_text = '\\n'.join([p.get_text() for p in paragraphs])\n",
    "\n",
    "            cna_data.append({'URLs': url, 'Text': extracted_text})\n",
    "            print(extracted_text[:60])\n",
    "        else:\n",
    "            print(f\"failed to retrieve {url}. status code: {response.status_code}\")\n",
    "            cna_data.append({'URLs': url, 'Text': ''})\n",
    "    except Exception as e:\n",
    "        print(f\"error fetching {url}: {str(e)}\")\n",
    "        cna_data.append({'URLs': url, 'Text': ''}) \n",
    "\n",
    "cna_text = pd.DataFrame(cna_data)\n",
    "\n",
    "cna_text.to_excel(\"text_scraping_urls.xlsx\", index=False)\n",
    "\n",
    "df_updated = pd.merge(df, cna_text, on='URLs', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Source</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.straitstimes.com/business/singapor...</td>\n",
       "      <td>straits times</td>\n",
       "      <td>2024 Q2</td>\n",
       "      <td>2024-05-10T07:24:00+08:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.straitstimes.com/business/singapor...</td>\n",
       "      <td>straits times</td>\n",
       "      <td>2024 Q2</td>\n",
       "      <td>2024-04-12T07:37:00+08:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.straitstimes.com/business/singapor...</td>\n",
       "      <td>straits times</td>\n",
       "      <td>2024 Q2</td>\n",
       "      <td>2024-05-23T08:00:00+08:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.straitstimes.com/business/singapor...</td>\n",
       "      <td>straits times</td>\n",
       "      <td>2024 Q2</td>\n",
       "      <td>2024-04-26T13:00:00+08:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.straitstimes.com/business/ramp-up-...</td>\n",
       "      <td>straits times</td>\n",
       "      <td>2024 Q2</td>\n",
       "      <td>2024-06-25T19:40:00+08:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>https://www.businesstimes.com.sg/property/new-...</td>\n",
       "      <td>business times</td>\n",
       "      <td>2020 Q1</td>\n",
       "      <td>2020-02-06T21:50:00+08:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>https://www.businesstimes.com.sg/property/coll...</td>\n",
       "      <td>business times</td>\n",
       "      <td>2020 Q1</td>\n",
       "      <td>2020-01-29T09:04:34+08:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>https://sbr.com.sg/manufacturing/news/seppure-...</td>\n",
       "      <td>singapore business review</td>\n",
       "      <td>2020 Q1</td>\n",
       "      <td>2020-01-08T14:24:50+0800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>https://sbr.com.sg/economy/news/inflation-rate...</td>\n",
       "      <td>singapore business review</td>\n",
       "      <td>2020 Q1</td>\n",
       "      <td>2020-01-23T17:29:36+0800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>https://sbr.com.sg/economy/in-focus/covid-19-p...</td>\n",
       "      <td>singapore business review</td>\n",
       "      <td>2020 Q1</td>\n",
       "      <td>2020-03-23T06:00:00+0800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URLs  \\\n",
       "0    https://www.straitstimes.com/business/singapor...   \n",
       "1    https://www.straitstimes.com/business/singapor...   \n",
       "2    https://www.straitstimes.com/business/singapor...   \n",
       "3    https://www.straitstimes.com/business/singapor...   \n",
       "4    https://www.straitstimes.com/business/ramp-up-...   \n",
       "..                                                 ...   \n",
       "302  https://www.businesstimes.com.sg/property/new-...   \n",
       "303  https://www.businesstimes.com.sg/property/coll...   \n",
       "304  https://sbr.com.sg/manufacturing/news/seppure-...   \n",
       "305  https://sbr.com.sg/economy/news/inflation-rate...   \n",
       "306  https://sbr.com.sg/economy/in-focus/covid-19-p...   \n",
       "\n",
       "                        Source  Quarter                       Date Text  \n",
       "0                straits times  2024 Q2  2024-05-10T07:24:00+08:00  NaN  \n",
       "1                straits times  2024 Q2  2024-04-12T07:37:00+08:00  NaN  \n",
       "2                straits times  2024 Q2  2024-05-23T08:00:00+08:00  NaN  \n",
       "3                straits times  2024 Q2  2024-04-26T13:00:00+08:00  NaN  \n",
       "4                straits times  2024 Q2  2024-06-25T19:40:00+08:00  NaN  \n",
       "..                         ...      ...                        ...  ...  \n",
       "302             business times  2020 Q1  2020-02-06T21:50:00+08:00  NaN  \n",
       "303             business times  2020 Q1  2020-01-29T09:04:34+08:00  NaN  \n",
       "304  singapore business review  2020 Q1   2020-01-08T14:24:50+0800  NaN  \n",
       "305  singapore business review  2020 Q1   2020-01-23T17:29:36+0800  NaN  \n",
       "306  singapore business review  2020 Q1   2020-03-23T06:00:00+0800  NaN  \n",
       "\n",
       "[307 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
